{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.lang.pt.examples import sentences \n",
    "from newspaper import Article, Source\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#spacy.cli.download(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm') # commented out once it's loaded\n",
    "\n",
    "punctuations = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newspapers to search\n",
    "\n",
    "newsp = ['cmjornal.pt/', \n",
    "         'dn.pt/',\n",
    "         'expresso.pt/',\n",
    "         'folhanacional.pt/',\n",
    "         'jn.pt/',\n",
    "         'ionline.sapo.pt/',   \n",
    "         'sol.sapo.pt/',\n",
    "         'observador.pt/',\n",
    "         'publico.pt/',\n",
    "         'sabado.pt/',\n",
    "         'sapo.pt/',\n",
    "         'visao.pt/',\n",
    "         ]\n",
    "\n",
    "not_keywords = [\"/multimedia\", \"/videos\", \n",
    "                \"/famosos\", \"/celebridades\", \n",
    "                \"/tecnologia\", \"/boa-vida\", \n",
    "                \"/tendencias\", \"/desporto\",\n",
    "                \"/maissobre\", \"/ciencia\",\n",
    "                \"/cinema\", \"/cultura\",\n",
    "                \"/cidades\", \"/dinheiro\",\n",
    "                \"/blogues\", \"/podcasts\",\n",
    "                \"/vida\", \"/artes\",\n",
    "                \"/iniciativas\", \"/colunistas/andre\",\n",
    "                \"/tribuna\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_json(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts 'url' and 'timestamp' from all JSON files in the specified folder and stores them in a DataFrame.\n",
    "    \n",
    "    :param folder_path: Path to the folder containing JSON files.\n",
    "    :return: A pandas DataFrame with columns 'url' and 'timestamp'.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a JSON file\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                try:\n",
    "                    json_data = json.load(file)\n",
    "                    # Check if the data is a list of dictionaries\n",
    "                    if isinstance(json_data, list):\n",
    "                        for entry in json_data:\n",
    "                            # Extract 'url' and 'timestamp' if available\n",
    "                            if 'url' in entry and 'timestamp' in entry:\n",
    "                                data.append({\n",
    "                                    'url': entry['url'],\n",
    "                                    'timestamp': entry['timestamp']\n",
    "                                })\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON file {file_name}: {e}\")\n",
    "\n",
    "    # Convert collected data into a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['url', 'timestamp'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_data_from_json(\"/Users/joaop.cardoso/MestradoCD/FCD/FDS_Project/cdx_results_json_files/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this command to avoid running the code above\n",
    "df = pd.read_csv('full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(df, keywords):\n",
    "    \"\"\"\n",
    "    Filters out rows from the DataFrame by assessing each keyword individually.\n",
    "    Returns the result as a dictionary.\n",
    "    \n",
    "    :param df: A pandas DataFrame with at least 'url' and 'timestamp' columns.\n",
    "    :param keywords: A list of keywords to filter out.\n",
    "    :return: A dictionary with 'url' and 'timestamp' for rows that do not contain any of the keywords.\n",
    "    \"\"\"\n",
    "    filtered_df = df.copy()\n",
    "\n",
    "    # Iterate over each keyword and filter rows\n",
    "    for keyword in keywords:\n",
    "        filtered_df = filtered_df[~filtered_df['url'].str.contains(keyword, case=False, na=False)]\n",
    "    \n",
    "    # Convert the filtered DataFrame to a dictionary\n",
    "    result_dict = pd.DataFrame(filtered_df)\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = filter_links(df, not_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the title\n",
    "def title_input(df): \n",
    "    processed_texts = []  # Initialize inside the function\n",
    "\n",
    "    # Extract the last part of each URL, handling NaN values\n",
    "    last_part = df['url'].str.rsplit('/').str[-1]  # Extract the last part of the URL\n",
    "    last_part = last_part.fillna(\"\")  # Replace NaN values with an empty string\n",
    "\n",
    "    # Handle cases where the last part is empty\n",
    "    last_part[last_part == \"\"] = df['url'].str.rsplit('/').str[-2].fillna(\"\")  # Use the second-to-last part if last is empty\n",
    "    \n",
    "    for part in last_part:\n",
    "        if \"-\" in part:\n",
    "            parts = part.rsplit('-')[0:-1]  # Split by '-' and remove the last element\n",
    "            sentence = \" \".join(parts)  # Join parts to form a sentence\n",
    "            processed_sentence = nlp(sentence)  # Process with SpaCy\n",
    "            processed_texts.append(\" \".join(token.text for token in processed_sentence))\n",
    "        else:\n",
    "            processed_texts.append(\"\")  # Append an empty string if no processing was done\n",
    "\n",
    "    # Use .loc to avoid SettingWithCopyWarning\n",
    "    df = df.copy()  # Create a copy to avoid SettingWithCopyWarning if df is a slice\n",
    "    df.loc[:, 'processed_url_text'] = processed_texts\n",
    "\n",
    "    # Update the original DataFrame dictionary with the filtered DataFrame\n",
    "    df_updated = df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = title_input(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the newspaper name in the URL\n",
    "def find_newspaper(url):\n",
    "    for newspaper in newsp:\n",
    "        if newspaper in url:\n",
    "            return newspaper\n",
    "    return None  # Return None if no newspaper is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['newspaper'] = df_clean['url'].apply(find_newspaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>processed_url_text</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cmjornal.pt/c-studio/especiais-c-s...</td>\n",
       "      <td>20200426174855</td>\n",
       "      <td>apoio domiciliario chega a 2200 pessoas por</td>\n",
       "      <td>cmjornal.pt/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.cmjornal.pt/c-studio/especiais-c-s...</td>\n",
       "      <td>20200427174619</td>\n",
       "      <td>apoio domiciliario chega a 2200 pessoas por</td>\n",
       "      <td>cmjornal.pt/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cmjornal.pt/c-studio/especiais-c-s...</td>\n",
       "      <td>20200428171922</td>\n",
       "      <td>apoio domiciliario chega a 2200 pessoas por</td>\n",
       "      <td>cmjornal.pt/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cmjornal.pt/c-studio/especiais-c-s...</td>\n",
       "      <td>20200429172316</td>\n",
       "      <td>apoio domiciliario chega a 2200 pessoas por</td>\n",
       "      <td>cmjornal.pt/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.cmjornal.pt/c-studio/especiais-c-s...</td>\n",
       "      <td>20200430181128</td>\n",
       "      <td>apoio domiciliario chega a 2200 pessoas por</td>\n",
       "      <td>cmjornal.pt/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url       timestamp  \\\n",
       "0  https://www.cmjornal.pt/c-studio/especiais-c-s...  20200426174855   \n",
       "1  https://www.cmjornal.pt/c-studio/especiais-c-s...  20200427174619   \n",
       "2  https://www.cmjornal.pt/c-studio/especiais-c-s...  20200428171922   \n",
       "3  https://www.cmjornal.pt/c-studio/especiais-c-s...  20200429172316   \n",
       "4  https://www.cmjornal.pt/c-studio/especiais-c-s...  20200430181128   \n",
       "\n",
       "                            processed_url_text     newspaper  \n",
       "0  apoio domiciliario chega a 2200 pessoas por  cmjornal.pt/  \n",
       "1  apoio domiciliario chega a 2200 pessoas por  cmjornal.pt/  \n",
       "2  apoio domiciliario chega a 2200 pessoas por  cmjornal.pt/  \n",
       "3  apoio domiciliario chega a 2200 pessoas por  cmjornal.pt/  \n",
       "4  apoio domiciliario chega a 2200 pessoas por  cmjornal.pt/  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_url_and_timestamp(row):\n",
    "    \"\"\"\n",
    "    Combines the base URL with the 'url' and 'timestamp' columns from a DataFrame row.\n",
    "    \n",
    "    :param row: A row from a pandas DataFrame.\n",
    "    :return: A combined URL string.\n",
    "    \"\"\"\n",
    "    base_url = \"https://arquivo.pt/wayback/\"\n",
    "    return f\"{base_url}{row['timestamp']}/{row['url']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['arquivo_url'] = df_clean.apply(combine_url_and_timestamp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the dataframe, for links with \"chega\" and \"andre ventura\"\n",
    "def filter_dataframe(df, text_column=\"processed_url_text\"):\n",
    "    # List to keep track of row indices that meet the criteria\n",
    "    indices_to_keep = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame to access both the index and text\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[text_column]\n",
    "        \n",
    "        # Skip if the text is NaN\n",
    "        if pd.isna(text):\n",
    "            continue\n",
    "        \n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Check if \"chega\" appears as a noun in the document\n",
    "        is_chega_noun = any(token.text.lower() == \"chega\" and token.pos_ == \"NOUN\" for token in doc)\n",
    "\n",
    "        # Check if both \"andre\" and \"ventura\" appear in the document\n",
    "        contains_andre_ventura = \"andre\" in text.lower() and \"ventura\" in text.lower()\n",
    "\n",
    "        # If either condition is met, keep the row index\n",
    "        if is_chega_noun or contains_andre_ventura:\n",
    "            indices_to_keep.append(index)\n",
    "\n",
    "    # Filter the DataFrame to only include rows that meet the criteria\n",
    "    df = df.loc[indices_to_keep].reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = filter_dataframe(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_clean.to_csv('full_ds_title_clean.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from this point\n",
    "df_clean = pd.read_csv('full_ds_title_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_work.to_csv('working_dataset.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: André Ventura alega estar impedido de fazer campanha e alerta para risco de perda de mandato\n",
      "Authors: ['Sofia Rodrigues']\n",
      "Publish Date: 2020-12-31 00:00:00\n",
      "Text: O deputado e candidato presidencial do Chega, André Ventura, interpôs junto do Supremo Tribunal Administrativo (STA) uma acção de intimação de comportamento ao presidente da Assembleia da República (AR), Eduardo Ferro Rodrigues, para que permita a suspensão do seu mandato parlamentar e a substituição por outro elemento do partido nas próximas semanas.\n",
      "\n",
      "No processo a que o PÚBLICO teve acesso, o candidato à Presidência da República diz que está “impedido de poder exercer o seu direito a fazer cam\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape and parse an article\n",
    "def scrape_article(url):\n",
    "    \"\"\"\n",
    "    Fetches and parses the article content from the given URL.\n",
    "\n",
    "    :param url: URL of the article.\n",
    "    :return: A dictionary with the article's title, authors, publish date, and text.\n",
    "    \"\"\"\n",
    "    article = Article(url)\n",
    "    article.download()  # Download the article content\n",
    "    article.parse()     # Parse the downloaded content\n",
    "\n",
    "    return {\n",
    "        \"title\": article.title,\n",
    "        \"authors\": article.authors,\n",
    "        \"publish_date\": article.publish_date,\n",
    "        \"text\": article.text\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "article_url = \"https://www.publico.pt/2020/12/31/politica/noticia/andre-ventura-alega-estar-impedido-campanha-alerta-risco-perda-mandato-1944739\"\n",
    "article_data = scrape_article(article_url)\n",
    "\n",
    "print(f\"Title: {article_data['title']}\")\n",
    "print(f\"Authors: {article_data['authors']}\")\n",
    "print(f\"Publish Date: {article_data['publish_date']}\")\n",
    "print(f\"Text: {article_data['text'][:500]}\")  # Print the first 500 characters of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the content of an Arquivo.pt link\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=50)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Find all <p> tags and concatenate their text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = \" \".join([p.get_text() for p in paragraphs])\n",
    "        \n",
    "        # Return the article text or None if no <p> elements are found\n",
    "        return article_text if article_text.strip() else None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the content of an Arquivo.pt link\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=50)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Find all <p> tags and concatenate their text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = \" \".join([p.get_text() for p in paragraphs])\n",
    "        \n",
    "        # Return the article text or None if no <p> elements are found\n",
    "        return article_text if article_text.strip() else None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "updated_dfs = {}\n",
    "# Process each filtered DataFrame and update with Arquivo links and article content\n",
    "for y in years:\n",
    "    df = filtered_dfs[f\"filtered_df_{y}\"]\n",
    "\n",
    "    # Generate Arquivo link for each row and scrape article content\n",
    "    df['arquivo_url'] = df.apply(generate_arquivo_link, axis=1)\n",
    "    df['article_text'] = df['url'].apply(scrape_article_content)\n",
    "\n",
    "    # Optional: Add delay to avoid overloading the server and getting blocked\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Update the filtered DataFrame in the dictionary\n",
    "    updated_dfs[f\"filtered_df_{y}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionary to introduce the tokens\n",
    "lemmas_dict = {}\n",
    "\n",
    "stop_words = STOP_WORDS\n",
    "\n",
    "# Loop through each review along with its index\n",
    "for index, review in enumerate(updated_dfs['filtered_df_2020-2021']['article_text']):\n",
    "    if review is None:\n",
    "        print(f\"Skipping index {index} because the review is None\")\n",
    "        continue\n",
    "    doc = nlp(review)\n",
    "    lemmas = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc ]\n",
    "    lemmas = [ word for word in lemmas if word not in stop_words and word not in punctuations ]\n",
    "    lemmas_dict[index] = lemmas  # Store lemmas in the dictionary with index as key\n",
    "\n",
    "# Convert dictionary to a series and assign as a new column in the DataFrame\n",
    "updated_dfs['filtered_df_2020-2021']['Lemmas'] = pd.Series(lemmas_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dfs['filtered_df_2020-2021']['Lemmas']\n",
    "filtered_df = updated_dfs['filtered_df_2020-2021'].dropna(subset=['article_text'])\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "def w_counter(words):\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common()\n",
    "    word_freq_dict = {\"Word\": [word for word, freq in common_words], \"Frequency\": [freq for word, freq in common_words]}\n",
    "    return word_freq_dict\n",
    "\n",
    "all_words = [word for sublist in filtered_df['Lemmas'] for word in sublist]\n",
    "\n",
    "word_freq = pd.DataFrame(w_counter(all_words))\n",
    "\n",
    "# Select the top 10 words by frequency\n",
    "top_10_words = word_freq.head(10)\n",
    "\n",
    "# Plot the top 10 words\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_words['Word'], top_10_words['Frequency'], color='skyblue')\n",
    "plt.title(\"Top 10 Most Frequently Used Words\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(word_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
