{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook only for trying how to use NLP/LLM Models to understand text better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text for tokenization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/errors.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/compat.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/config.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/types.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     Any,\n\u001b[1;32m      6\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     overload,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/compat.py:99\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m has_mxnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     h5py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/h5py/__init__.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# --- Library setup -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# When importing from the root of the unpacked tarball or git checkout,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Python sees the \"h5py\" source directory and tries to load it, which fails.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# We tried working around this by using \"package_dir\" but that breaks Cython.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _errors\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_op\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_errors.pyx:1\u001b[0m, in \u001b[0;36minit h5py._errors\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This is a sample text for tokenization.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information from ChatGPT about possible usable Tools\n",
    "\n",
    "\n",
    "Um einen Text nach dem Web-Scraping zu analysieren und zu verstehen, gibt es verschiedene Ans√§tze, die sowohl auf klassischen NLP-Techniken (Natural Language Processing) als auch auf modernen LLM-Modellen (Large Language Models) basieren. Diese Tools und Methoden helfen dir, den Inhalt eines Textes zu verstehen, Schl√ºsselthemen zu identifizieren, Stimmungen zu erfassen oder bestimmte Entit√§ten zu extrahieren.\n",
    "\n",
    "Hier gebe ich dir einen √úberblick √ºber verschiedene M√∂glichkeiten, wie du Texte analysieren kannst, und erl√§utere, wie moderne LLMs wie GPT (Generative Pre-trained Transformer) eingesetzt werden k√∂nnen:\n",
    "\n",
    "1. Grundlegende NLP-Techniken\n",
    "Diese Techniken k√∂nnen verwendet werden, um Texte mit klassischen NLP-Bibliotheken wie spaCy, NLTK oder transformers (Hugging Face) zu analysieren.\n",
    "\n",
    "a. Tokenisierung\n",
    "\n",
    "Was es ist: Der Prozess der Aufteilung eines Textes in kleinere Einheiten, sogenannte Token (W√∂rter, S√§tze).\n",
    "Verwendung: Um zu verstehen, aus welchen W√∂rtern oder Phrasen ein Text besteht.\n",
    "Tools: spaCy, NLTK, transformers\n",
    "Beispiel (mit spaCy):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This is a sample text for tokenization.\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "b. POS-Tagging (Part-of-Speech)\n",
    "\n",
    "Was es ist: Erkennung der Wortart jedes Tokens (z. B. Nomen, Verb, Adjektiv).\n",
    "Verwendung: Hilft zu verstehen, welche Rolle jedes Wort im Satz spielt.\n",
    "Tools: spaCy, NLTK\n",
    "Beispiel:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "c. Named Entity Recognition (NER)\n",
    "\n",
    "Was es ist: Erkennung von Entit√§ten im Text (z. B. Namen von Personen, Orten, Organisationen, Zeitangaben).\n",
    "Verwendung: Um wichtige Entit√§ten im Text zu extrahieren.\n",
    "Tools: spaCy, transformers\n",
    "Beispiel:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "d. Sentiment-Analyse\n",
    "\n",
    "Was es ist: Bestimmung der Stimmung des Textes (positiv, negativ, neutral).\n",
    "Verwendung: Um die allgemeine Stimmung in einem Text oder Artikel zu erkennen.\n",
    "Tools: TextBlob, VADER, transformers\n",
    "Beispiel (mit TextBlob):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from textblob import TextBlob\n",
    "text = \"This is a fantastic product!\"\n",
    "analysis = TextBlob(text)\n",
    "print(analysis.sentiment)\n",
    "e. Stemming und Lemmatisierung\n",
    "\n",
    "Was es ist: Reduzierung von W√∂rtern auf ihre Grundformen.\n",
    "Verwendung: Um verschiedene Formen eines Wortes (z. B. ‚Äûl√§uft‚Äú, ‚Äûlaufen‚Äú) zu erkennen.\n",
    "Tools: spaCy, NLTK\n",
    "Beispiel (Lemmatisierung):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "2. Fortgeschrittene Methoden: LLMs (Large Language Models)\n",
    "Moderne Large Language Models (LLMs) wie GPT-3 oder GPT-4, die von OpenAI entwickelt wurden, sowie Modelle, die mit Hugging Face verf√ºgbar sind, bieten eine weit tiefere Analyse, indem sie nicht nur grundlegende NLP-Techniken anwenden, sondern auch semantisches Verst√§ndnis und Kontextinterpretation erm√∂glichen.\n",
    "\n",
    "a. Textzusammenfassung\n",
    "\n",
    "Was es ist: Automatische Erstellung einer kurzen Zusammenfassung des Textes.\n",
    "Verwendung: Um schnell den Inhalt langer Texte zu erfassen.\n",
    "Tools: Hugging Face Transformers, GPT-3, GPT-4\n",
    "Beispiel (mit Hugging Face):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "text = \"Long article goes here...\"\n",
    "summary = summarizer(text, max_length=100, min_length=30)\n",
    "print(summary[0]['summary_text'])\n",
    "b. Fragen zum Text beantworten (Question Answering)\n",
    "\n",
    "Was es ist: Ein System, das in der Lage ist, Fragen auf der Grundlage eines Textes zu beantworten.\n",
    "Verwendung: Um gezielt nach Informationen in einem Text zu suchen.\n",
    "Tools: GPT-3, Hugging Face (BERT-basierte Modelle)\n",
    "Beispiel (mit Hugging Face):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"The Eiffel Tower is located in Paris.\"\n",
    "result = qa_pipeline(question=\"Where is the Eiffel Tower?\", context=context)\n",
    "print(result['answer'])\n",
    "c. Themenmodellierung (Topic Modeling)\n",
    "\n",
    "Was es ist: Identifizierung der Hauptthemen, die in einem Text besprochen werden.\n",
    "Verwendung: Um zu verstehen, welche Themen in gro√üen Textmengen behandelt werden.\n",
    "Tools: Gensim, Latent Dirichlet Allocation (LDA)\n",
    "Beispiel (mit Gensim):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from gensim import corpora, models\n",
    "# List of documents (texts)\n",
    "texts = [['human', 'interface', 'computer'], ['survey', 'user', 'computer']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "print(lda.print_topics())\n",
    "d. LLM-basierte Textinterpretation\n",
    "\n",
    "Was es ist: Gro√üe Sprachmodelle wie GPT-3 oder GPT-4 verstehen den Text als Ganzes, erkennen den Kontext und k√∂nnen den Text interpretieren oder sogar neue Inhalte basierend auf dem Text generieren.\n",
    "Verwendung: Um detaillierte Analysen oder kreative Anwendungen zu erstellen, z. B. Zusammenfassungen, Interpretation von Argumenten oder Vorhersagen auf der Grundlage von Textinhalten.\n",
    "Tools: OpenAI GPT-3/GPT-4 API\n",
    "Beispiel (mit OpenAI GPT-3/4):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key-here'\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  engine=\"text-davinci-003\",\n",
    "  prompt=\"Explain the main points of the following article: ...\",\n",
    "  max_tokens=150\n",
    ")\n",
    "print(response.choices[0].text.strip())\n",
    "3. Kombination von Web-Scraping und Textanalyse\n",
    "Du kannst Web-Scraping und NLP-Modelle oder LLMs kombinieren, um automatisch Texte zu extrahieren und zu analysieren. Ein m√∂gliches Workflow-Szenario k√∂nnte so aussehen:\n",
    "\n",
    "Web Scraping: Scrape Inhalte von Nachrichtenwebseiten (z. B. mit requests und BeautifulSoup).\n",
    "Textanalyse:\n",
    "Verwende grundlegende NLP-Tools (z. B. spaCy), um Entit√§ten, Schl√ºsselw√∂rter oder Stimmungen zu erkennen.\n",
    "Nutze LLMs (z. B. GPT-3), um tieferes Verst√§ndnis, Zusammenfassungen oder thematische Erkennung zu erm√∂glichen.\n",
    "Beispiel-Workflow:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "# Schritt 1: Web Scraping\n",
    "url = 'https://example.com/news-article'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "article_text = soup.find('div', class_='article-content').get_text()\n",
    "\n",
    "# Schritt 2: Zusammenfassung mit Hugging Face\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summary = summarizer(article_text, max_length=100, min_length=30)\n",
    "print(\"Zusammenfassung:\", summary[0]['summary_text'])\n",
    "4. Zusammenfassung\n",
    "Klassische NLP-Ans√§tze wie Tokenisierung, POS-Tagging und Named Entity Recognition bieten grundlegende Tools, um Texte zu analysieren und Informationen zu extrahieren.\n",
    "Moderne LLMs wie GPT-3/GPT-4 erm√∂glichen tiefere semantische Analysen, Textverst√§ndnis und Textgenerierung.\n",
    "Du kannst Web-Scraping nutzen, um Daten aus dem Web zu extrahieren, und anschlie√üend NLP-Tools und LLMs verwenden, um die Inhalte zu analysieren und zu verstehen.\n",
    "Dies bietet eine Grundlage, um verschiedene Analysemethoden in euer Projekt einzubauen, abh√§ngig davon, wie tief die Analyse und das Textverst√§ndnis sein sollen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
