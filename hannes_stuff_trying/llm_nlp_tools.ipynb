{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook only for trying how to use NLP/LLM Models to understand text better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text for tokenization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/errors.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/compat.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/config.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/types.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     Any,\n\u001b[1;32m      6\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     overload,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy, has_cupy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     28\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/thinc/compat.py:99\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m has_mxnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     h5py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/h5py/__init__.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# --- Library setup -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# When importing from the root of the unpacked tarball or git checkout,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Python sees the \"h5py\" source directory and tries to load it, which fails.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# We tried working around this by using \"package_dir\" but that breaks Cython.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _errors\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_op\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_errors.pyx:1\u001b[0m, in \u001b[0;36minit h5py._errors\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This is a sample text for tokenization.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information from ChatGPT about possible usable Tools\n",
    "\n",
    "\n",
    "Um einen Text nach dem Web-Scraping zu analysieren und zu verstehen, gibt es verschiedene Ansätze, die sowohl auf klassischen NLP-Techniken (Natural Language Processing) als auch auf modernen LLM-Modellen (Large Language Models) basieren. Diese Tools und Methoden helfen dir, den Inhalt eines Textes zu verstehen, Schlüsselthemen zu identifizieren, Stimmungen zu erfassen oder bestimmte Entitäten zu extrahieren.\n",
    "\n",
    "Hier gebe ich dir einen Überblick über verschiedene Möglichkeiten, wie du Texte analysieren kannst, und erläutere, wie moderne LLMs wie GPT (Generative Pre-trained Transformer) eingesetzt werden können:\n",
    "\n",
    "1. Grundlegende NLP-Techniken\n",
    "Diese Techniken können verwendet werden, um Texte mit klassischen NLP-Bibliotheken wie spaCy, NLTK oder transformers (Hugging Face) zu analysieren.\n",
    "\n",
    "a. Tokenisierung\n",
    "\n",
    "Was es ist: Der Prozess der Aufteilung eines Textes in kleinere Einheiten, sogenannte Token (Wörter, Sätze).\n",
    "Verwendung: Um zu verstehen, aus welchen Wörtern oder Phrasen ein Text besteht.\n",
    "Tools: spaCy, NLTK, transformers\n",
    "Beispiel (mit spaCy):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This is a sample text for tokenization.\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "b. POS-Tagging (Part-of-Speech)\n",
    "\n",
    "Was es ist: Erkennung der Wortart jedes Tokens (z. B. Nomen, Verb, Adjektiv).\n",
    "Verwendung: Hilft zu verstehen, welche Rolle jedes Wort im Satz spielt.\n",
    "Tools: spaCy, NLTK\n",
    "Beispiel:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "c. Named Entity Recognition (NER)\n",
    "\n",
    "Was es ist: Erkennung von Entitäten im Text (z. B. Namen von Personen, Orten, Organisationen, Zeitangaben).\n",
    "Verwendung: Um wichtige Entitäten im Text zu extrahieren.\n",
    "Tools: spaCy, transformers\n",
    "Beispiel:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "d. Sentiment-Analyse\n",
    "\n",
    "Was es ist: Bestimmung der Stimmung des Textes (positiv, negativ, neutral).\n",
    "Verwendung: Um die allgemeine Stimmung in einem Text oder Artikel zu erkennen.\n",
    "Tools: TextBlob, VADER, transformers\n",
    "Beispiel (mit TextBlob):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from textblob import TextBlob\n",
    "text = \"This is a fantastic product!\"\n",
    "analysis = TextBlob(text)\n",
    "print(analysis.sentiment)\n",
    "e. Stemming und Lemmatisierung\n",
    "\n",
    "Was es ist: Reduzierung von Wörtern auf ihre Grundformen.\n",
    "Verwendung: Um verschiedene Formen eines Wortes (z. B. „läuft“, „laufen“) zu erkennen.\n",
    "Tools: spaCy, NLTK\n",
    "Beispiel (Lemmatisierung):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "2. Fortgeschrittene Methoden: LLMs (Large Language Models)\n",
    "Moderne Large Language Models (LLMs) wie GPT-3 oder GPT-4, die von OpenAI entwickelt wurden, sowie Modelle, die mit Hugging Face verfügbar sind, bieten eine weit tiefere Analyse, indem sie nicht nur grundlegende NLP-Techniken anwenden, sondern auch semantisches Verständnis und Kontextinterpretation ermöglichen.\n",
    "\n",
    "a. Textzusammenfassung\n",
    "\n",
    "Was es ist: Automatische Erstellung einer kurzen Zusammenfassung des Textes.\n",
    "Verwendung: Um schnell den Inhalt langer Texte zu erfassen.\n",
    "Tools: Hugging Face Transformers, GPT-3, GPT-4\n",
    "Beispiel (mit Hugging Face):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "text = \"Long article goes here...\"\n",
    "summary = summarizer(text, max_length=100, min_length=30)\n",
    "print(summary[0]['summary_text'])\n",
    "b. Fragen zum Text beantworten (Question Answering)\n",
    "\n",
    "Was es ist: Ein System, das in der Lage ist, Fragen auf der Grundlage eines Textes zu beantworten.\n",
    "Verwendung: Um gezielt nach Informationen in einem Text zu suchen.\n",
    "Tools: GPT-3, Hugging Face (BERT-basierte Modelle)\n",
    "Beispiel (mit Hugging Face):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"The Eiffel Tower is located in Paris.\"\n",
    "result = qa_pipeline(question=\"Where is the Eiffel Tower?\", context=context)\n",
    "print(result['answer'])\n",
    "c. Themenmodellierung (Topic Modeling)\n",
    "\n",
    "Was es ist: Identifizierung der Hauptthemen, die in einem Text besprochen werden.\n",
    "Verwendung: Um zu verstehen, welche Themen in großen Textmengen behandelt werden.\n",
    "Tools: Gensim, Latent Dirichlet Allocation (LDA)\n",
    "Beispiel (mit Gensim):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from gensim import corpora, models\n",
    "# List of documents (texts)\n",
    "texts = [['human', 'interface', 'computer'], ['survey', 'user', 'computer']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "print(lda.print_topics())\n",
    "d. LLM-basierte Textinterpretation\n",
    "\n",
    "Was es ist: Große Sprachmodelle wie GPT-3 oder GPT-4 verstehen den Text als Ganzes, erkennen den Kontext und können den Text interpretieren oder sogar neue Inhalte basierend auf dem Text generieren.\n",
    "Verwendung: Um detaillierte Analysen oder kreative Anwendungen zu erstellen, z. B. Zusammenfassungen, Interpretation von Argumenten oder Vorhersagen auf der Grundlage von Textinhalten.\n",
    "Tools: OpenAI GPT-3/GPT-4 API\n",
    "Beispiel (mit OpenAI GPT-3/4):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key-here'\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  engine=\"text-davinci-003\",\n",
    "  prompt=\"Explain the main points of the following article: ...\",\n",
    "  max_tokens=150\n",
    ")\n",
    "print(response.choices[0].text.strip())\n",
    "3. Kombination von Web-Scraping und Textanalyse\n",
    "Du kannst Web-Scraping und NLP-Modelle oder LLMs kombinieren, um automatisch Texte zu extrahieren und zu analysieren. Ein mögliches Workflow-Szenario könnte so aussehen:\n",
    "\n",
    "Web Scraping: Scrape Inhalte von Nachrichtenwebseiten (z. B. mit requests und BeautifulSoup).\n",
    "Textanalyse:\n",
    "Verwende grundlegende NLP-Tools (z. B. spaCy), um Entitäten, Schlüsselwörter oder Stimmungen zu erkennen.\n",
    "Nutze LLMs (z. B. GPT-3), um tieferes Verständnis, Zusammenfassungen oder thematische Erkennung zu ermöglichen.\n",
    "Beispiel-Workflow:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "# Schritt 1: Web Scraping\n",
    "url = 'https://example.com/news-article'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "article_text = soup.find('div', class_='article-content').get_text()\n",
    "\n",
    "# Schritt 2: Zusammenfassung mit Hugging Face\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summary = summarizer(article_text, max_length=100, min_length=30)\n",
    "print(\"Zusammenfassung:\", summary[0]['summary_text'])\n",
    "4. Zusammenfassung\n",
    "Klassische NLP-Ansätze wie Tokenisierung, POS-Tagging und Named Entity Recognition bieten grundlegende Tools, um Texte zu analysieren und Informationen zu extrahieren.\n",
    "Moderne LLMs wie GPT-3/GPT-4 ermöglichen tiefere semantische Analysen, Textverständnis und Textgenerierung.\n",
    "Du kannst Web-Scraping nutzen, um Daten aus dem Web zu extrahieren, und anschließend NLP-Tools und LLMs verwenden, um die Inhalte zu analysieren und zu verstehen.\n",
    "Dies bietet eine Grundlage, um verschiedene Analysemethoden in euer Projekt einzubauen, abhängig davon, wie tief die Analyse und das Textverständnis sein sollen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
