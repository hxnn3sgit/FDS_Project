{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving words from newspaper titles in textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cdx_results_2019.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m json_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cdx_results_2019.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Open and load the JSON file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# List to store the URLs\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cdx_results_2019.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = './cdx_results_2019.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# List to store the URLs\n",
    "urls = []\n",
    "\n",
    "# Loop through the data and extract 'url' values\n",
    "for entry in data:\n",
    "    if 'url' in entry:  # Check if 'url' key exists\n",
    "        urls.append(entry['url'])  # Add the URL to the list\n",
    "\n",
    "# Open a file to write the results\n",
    "with open(\"titles.txt\", \"w\") as file:\n",
    "    # Loop through each URL\n",
    "    for url in urls:\n",
    "        # Remove 'https://' and split the URL at slashes\n",
    "        parts = url.split('/')\n",
    "        \n",
    "        # Extract the last part after the slash, which usually contains the title\n",
    "        title_with_params = parts[-1]\n",
    "        \n",
    "        # Remove any parameters (everything after '?')\n",
    "        title = title_with_params.split('?')[0]\n",
    "        \n",
    "        # Replace hyphens with spaces to improve readability\n",
    "        formatted_title = title.replace('-', ' ')\n",
    "        \n",
    "        # Write the formatted title to the file\n",
    "        file.write(f\"{formatted_title}\\n\")\n",
    "\n",
    "print(\"Titles extracted and written to 'titles.txt'.\")\n",
    "\n",
    "# maybe store urls and words wich belong togethers in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cdx_results_json_files/cdx_results_2019.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m json_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cdx_results_json_files/cdx_results_2019.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Open and load the JSON file\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     25\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Empty dictionary to store the URLs as keys and the split title words as values\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cdx_results_json_files/cdx_results_2019.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to split the URL into words from the title\n",
    "def split_url_in_words(url):\n",
    "    # Remove 'https://' and split the URL at slashes\n",
    "    parts = url.split('/')\n",
    "    \n",
    "    # Extract the last part after the slash, which usually contains the title\n",
    "    title_with_params = parts[-1]\n",
    "    \n",
    "    # Remove any parameters (everything after '?')\n",
    "    title = title_with_params.split('?')[0]\n",
    "    \n",
    "    # Replace hyphens with spaces to improve readability\n",
    "    formatted_title = title.replace('-', ' ')\n",
    "    \n",
    "    # Return the split words\n",
    "    return formatted_title\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = './cdx_results_json_files/cdx_results_2019.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Empty dictionary to store the URLs as keys and the split title words as values\n",
    "urls = {}\n",
    "\n",
    "# Iterate through the JSON data\n",
    "for entry in data:\n",
    "    if 'url' in entry:\n",
    "        #url = entry['url']  # Get the full URL\n",
    "        #split_title = split_url_in_words(url)  # Split the URL title into words\n",
    "        #urls[str(entry['url'])] = split_title  # Store the URL as key and split title as value\n",
    "        urls[str(entry['url'])] = split_url_in_words(entry['url'])\n",
    "\n",
    "# Writing dictionary to a textfile\n",
    "with open('urls_titles.json', 'w') as output_file:\n",
    "    json.dump(urls, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trying to scrape only the < title > elements in the links and look up if Chega is written as: 'Chega' or 'CHEGA':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Path to the JSON file\n",
    "file_name = 'cdx_results_2019.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "titles = {}\n",
    "\n",
    "# Loop through URLs; search for the title element in HTML-Script\n",
    "for v in data:\n",
    "    url = v['url']\n",
    "    \n",
    "    try:\n",
    "        # Requesting the website and setting a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10, stream=True) # stream=True for downloading data in chunks not everything at once \n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(1)  # 1-second delay\n",
    "\n",
    "        # Initialize an empty content variable and stream content chunks\n",
    "        html_content = b\"\"\n",
    "        \n",
    "        # Stream the content until we find the closing </title> tag\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            html_content += chunk\n",
    "            if b\"</title>\" in html_content:\n",
    "                break  # Stop streaming once the <title> tag is found\n",
    "\n",
    "        # Parse only the partial content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Extract the <title> element\n",
    "        title_tag = soup.find(\"title\")\n",
    "\n",
    "        # If no <title> tag is found, skip this page, makes program faster\n",
    "        if not title_tag:\n",
    "            continue\n",
    "\n",
    "        # Extract the text from the <title> tag\n",
    "        title_text = title_tag.get_text()\n",
    "\n",
    "        # Check for \"Chega\" or \"CHEGA\" (case-sensitive check)\n",
    "        if \"Chega\" in title_text or \"CHEGA\" or \"Andre Ventura\" in title_text:\n",
    "            titles[url] = title_text  # Store the URL and title in the dictionary\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for URL: {url}\")\n",
    "        continue  # Skip to the next URL if a timeout occurs\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for URL: {url} with error: {e}\")\n",
    "        continue  # Skip to the next URL if another error occurs\n",
    "\n",
    "# Writing the dictionary to a JSON file\n",
    "with open('titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(titles, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print the results, only for debugging, to remove later\n",
    "for url, title in titles.items():\n",
    "    print(url, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in 1st delivery:\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Path to the JSON file\n",
    "file_name = 'cdx_results_2019.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "titles = {}\n",
    "\n",
    "# Loop through URLs; search for the title element in HTML-Script\n",
    "counter = 0\n",
    "\n",
    "for v in data:\n",
    "    url = v['url']\n",
    "    print(f'iteration {counter}')\n",
    "    print(f'link: {url}')\n",
    "    counter += 1\n",
    "    \n",
    "    try:\n",
    "        # Requesting the website and setting a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10, stream=True) # stream=True for downloading data in chunks not everything at once \n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        # time.sleep(1)  # 1-second delay; should may be in code if server gets too much requests, for now works fine without delay\n",
    "\n",
    "        # Initialize an empty content variable and stream content chunks\n",
    "        html_content = b\"\"\n",
    "        \n",
    "        # Stream the content until we find the closing </title> tag\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            html_content += chunk\n",
    "            if b\"</title>\" in html_content:\n",
    "                break  # Stop streaming once the <title> tag is found\n",
    "\n",
    "        # Parse only the partial content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\") # optional lxml parser\n",
    "\n",
    "        # Extract the <title> element\n",
    "        title_tag = soup.find(\"title\")\n",
    "\n",
    "        # If no <title> tag is found, skip this page, makes program faster\n",
    "        if not title_tag:\n",
    "            continue\n",
    "\n",
    "        # Extract the text from the <title> tag\n",
    "        title_text = title_tag.get_text()\n",
    "\n",
    "        # Check for \"Chega\" or \"CHEGA\" (case-sensitive check)\n",
    "        if \"Chega\" in title_text or \"CHEGA\" or \"Andre Ventura\" in title_text:\n",
    "            print(f'title text \"{title_text}\" is valid\"')\n",
    "            titles[url] = title_text  # Store the URL and title in the dictionary\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for URL: {url}\")\n",
    "        continue  # Skip to the next URL if a timeout occurs\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for URL: {url} with error: {e}\")\n",
    "        continue  # Skip to the next URL if another error occurs\n",
    "\n",
    "# Writing the dictionary to a JSON file\n",
    "with open('titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(titles, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Debug output:\n",
    "\n",
    "for url, title in titles.items():\n",
    "    print(f'{url}: {title}')\n",
    "\n",
    "print(titles)\n",
    "\n",
    "#df_titles = pd.DataFrame(data=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine all titles into one string\n",
    "all_titles = ' '.join(titles.values())\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_titles)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # No axes for a clean look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Path to the JSON file\n",
    "file_name = 'cdx_results_2019.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "titles = {}\n",
    "\n",
    "# Loop through URLs; search for the title element in HTML-Script\n",
    "counter = 0\n",
    "\n",
    "for v in data:\n",
    "    url = v['url']\n",
    "    print(f'iteration {counter}')\n",
    "    print(f'link: {url}')\n",
    "    counter += 1\n",
    "    \n",
    "    try:\n",
    "        # Requesting the website and setting a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10, stream=True) # stream=True for downloading data in chunks not everything at once \n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        #time.sleep(1)  # 1-second delay\n",
    "\n",
    "        # Initialize an empty content variable and stream content chunks\n",
    "        html_content = b\"\"\n",
    "        \n",
    "        # Stream the content until we find the closing </title> tag\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            html_content += chunk\n",
    "            if b\"</title>\" in html_content:\n",
    "                break  # Stop streaming once the <title> tag is found\n",
    "\n",
    "        # Parse only the partial content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Extract the <title> element\n",
    "        title_tag = soup.find(\"title\")\n",
    "\n",
    "        # If no <title> tag is found, skip this page, makes program faster\n",
    "        if not title_tag:\n",
    "            continue\n",
    "\n",
    "        # Extract the text from the <title> tag\n",
    "        title_text = title_tag.get_text()\n",
    "\n",
    "        # Check for \"Chega\" or \"CHEGA\" (case-sensitive check)\n",
    "        if \"Chega\" in title_text or \"CHEGA\" in title_text:\n",
    "            print(f'title text \"{title_text} is valid\"')\n",
    "            titles[url] = title_text  # Store the URL and title in the dictionary\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for URL: {url}\")\n",
    "        continue  # Skip to the next URL if a timeout occurs\n",
    "\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for URL: {url} with error: {e}\")\n",
    "        continue  # Skip to the next URL if another error occurs\n",
    "\n",
    "# Writing the dictionary to a JSON file\n",
    "with open('titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(titsles, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the text too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
