{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial analysis of the collected data (number of counts, period for the timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "#spacy.cli.download(\"pt_core_news_sm\")\n",
    "from spacy.lang.pt.examples import sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of occurrences of each entry and remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>status</th>\n",
       "      <th>url count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13877</th>\n",
       "      <td>https://www.sapo.pt/prime/article/fc-porto-che...</td>\n",
       "      <td>1970-08-22 16:33:44.171433</td>\n",
       "      <td>200</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13878</th>\n",
       "      <td>https://www.sapo.pt/prime/article/fc-porto-che...</td>\n",
       "      <td>1970-08-22 16:33:44.175956</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13895</th>\n",
       "      <td>http://www.sapo.pt/prime/article/fc-porto-cheg...</td>\n",
       "      <td>1970-08-22 16:35:02.011304</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13896</th>\n",
       "      <td>http://www.sapo.pt/prime/article/fc-porto-cheg...</td>\n",
       "      <td>1970-08-22 16:35:02.011308</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13898</th>\n",
       "      <td>http://www.sapo.pt/prime/article/fc-porto-cheg...</td>\n",
       "      <td>1970-08-22 16:35:02.021135</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "13877  https://www.sapo.pt/prime/article/fc-porto-che...   \n",
       "13878  https://www.sapo.pt/prime/article/fc-porto-che...   \n",
       "13895  http://www.sapo.pt/prime/article/fc-porto-cheg...   \n",
       "13896  http://www.sapo.pt/prime/article/fc-porto-cheg...   \n",
       "13898  http://www.sapo.pt/prime/article/fc-porto-cheg...   \n",
       "\n",
       "                       timestamp  status  url count  \n",
       "13877 1970-08-22 16:33:44.171433     200        127  \n",
       "13878 1970-08-22 16:33:44.175956     200          1  \n",
       "13895 1970-08-22 16:35:02.011304     200          2  \n",
       "13896 1970-08-22 16:35:02.011308     200          2  \n",
       "13898 1970-08-22 16:35:02.021135     200          2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to store the data analysed\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Newspaper\": [],\n",
    "    \"Number of occurences\": [],\n",
    "    \"Period\": []\n",
    "    })\n",
    "\n",
    "\n",
    "# Newspapers to search\n",
    "\n",
    "newsp = ['cmjornal.pt/', \n",
    "         'dn.pt/',\n",
    "         'expresso.pt/',\n",
    "         'folhanacional.pt/',\n",
    "         'jn.pt/',\n",
    "         'ionline.sapo.pt/',   \n",
    "         'sol.sapo.pt/',\n",
    "         'observador.pt/',\n",
    "         'publico.pt/',\n",
    "         'sabado.pt/',\n",
    "         'sapo.pt/',\n",
    "         'visao.pt/',\n",
    "         ]\n",
    "\n",
    "# Years of the analysis\n",
    "\n",
    "years = ['2019', '2020-2021', '2022-2024']\n",
    "\n",
    "# Counting the number of occurrences for each of the newspapers in the defined periods\n",
    "\n",
    "for y in years:\n",
    "    with open(\"/Users/joaop.cardoso/MestradoCD/FCD/FDS_Project/cdx_results_json_files/cdx_results_\"+y+\".json\", \"r\") as f:\n",
    "        data = f.read()\n",
    "        for i in newsp:\n",
    "                total = data.count(i)\n",
    "                df.loc[len(df)] = [i, total, y]\n",
    "\n",
    "# Count the number of occurrences of each item in the URL column and save it to the dataframe w/o the timestamp\n",
    "\n",
    "yearly_data = {}\n",
    "yearly_data_no_dupl = {}\n",
    "df_no_dupl = {}\n",
    "\n",
    "for y in years:\n",
    "    # Read the dataframe, count the number of URLs, merge the count to origina DF and then remove all duplicates based on URL\n",
    "    df = pd.read_json(\"/Users/joaop.cardoso/MestradoCD/FCD/FDS_Project/cdx_results_json_files/cdx_results_\"+y+\".json\")\n",
    "\n",
    "    # Count the repeated urls in each of the .json files\n",
    "    url_count = df.groupby(df['url']).size().reset_index(name = 'url count')\n",
    "\n",
    "    # Merge the column (list) of counted values per url into the original DF\n",
    "    yearly_data[f\"df_{y}\"] = df.merge(url_count, on = 'url', how = 'left')\n",
    "    \n",
    "    # Add the dataframes into a new dictionary\n",
    "    yearly_data_no_dupl[f\"df_{y}\"] = yearly_data[f\"df_{y}\"].drop_duplicates(subset = ['url'], keep = 'first')\n",
    "\n",
    "yearly_data_no_dupl['df_2019'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the title from the rest of the URL to enable text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https:', '', 'www.sapo.pt', 'noticias', 'nacional', 'ventura-cabeca-de-lista-do-chega-em-lisboa-e-_5d18ca3ad9048e7d170acddf']\n",
      "['ventura', 'cabeca', 'de', 'lista', 'do', 'chega', 'em', 'lisboa', 'e']\n",
      "ventura cabeca de lista do chega em lisboa e\n",
      "Word: ventura, POS: NOUN, Detailed Tag: NOUN\n",
      "Word: cabeca, POS: ADJ, Detailed Tag: ADJ\n",
      "Word: de, POS: ADP, Detailed Tag: ADP\n",
      "Word: lista, POS: NOUN, Detailed Tag: NOUN\n",
      "Word: do, POS: ADP, Detailed Tag: ADP\n",
      "Word: chega, POS: NOUN, Detailed Tag: NOUN\n",
      "Word: em, POS: ADP, Detailed Tag: ADP\n",
      "Word: lisboa, POS: PROPN, Detailed Tag: PROPN\n",
      "Word: e, POS: CCONJ, Detailed Tag: CCONJ\n"
     ]
    }
   ],
   "source": [
    "# Testing the methods for splitting text w/in URL, and analyzing the word 'chega'\n",
    "\n",
    "x = \"http://www.sapo.pt/prime/article/fc-porto-chega-ao-classico-na-luz-ensombrado-_5d60e7d99474b37d1ce81d86//\"\n",
    "\n",
    "x1 = \"https://www.sapo.pt/noticias/nacional/ventura-cabeca-de-lista-do-chega-em-lisboa-e-_5d18ca3ad9048e7d170acddf\"\n",
    "\n",
    "x2 = \"O presidente chega ao local.\"\n",
    "\n",
    "y_sent = []\n",
    "\n",
    "# The method rsplit splits the url string taking \"-\" as the delimiter. [1:-1] removes the first and last instance\n",
    "\n",
    "last_part = x1.rsplit('/')\n",
    "print(last_part)\n",
    "\n",
    "for i in last_part:\n",
    "    if \"-\" in i:\n",
    "        y = i.rsplit('-')[0:-1]\n",
    "        print(y)\n",
    "        y_sent = nlp(\" \".join(y))\n",
    "\n",
    "print(y_sent)\n",
    "for token in nlp.get_pipe(\"morphologizer\")(y_sent):\n",
    "    print(f\"Word: {token.text}, POS: {token.pos_}, Detailed Tag: {token.tag_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
