{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Deliverable for FCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data mining news articles related to André Ventura / CHEGA (potentially only headlines at first)\n",
    "\t- Timeframe 2019-2024\n",
    "2. Word cloud analysis since the party foundation, assess the topics of interest throughout the years\n",
    "3. Correlation with INE (National Institute of Statistics) data with the various topics (identify real trends, or made up trends by fake news/social media)\n",
    "4. Change of the party’s position regarding the identified topics\n",
    "5. Word clouds every 5 years of the portuguese politics landscape (2000-2005-2010-2015-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to make requests from ARQUIVO's CDX API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the first part of the script we defined the newspapers to search for (yes, CDX API makes you search by a *single* url at a time), and the function that will look them up.\n",
    "\n",
    "---\n",
    "### Challenges with the CDX API\n",
    "\n",
    "The CDX API can access any stored link in **arquivo API**, but it can only be filtered with some metadata and text within the URL\n",
    "- We made use of timestamps to limit our search to the period of interest. However,  it doesn't mean that all the pages collected are from that period, as there are several snapshots from pages way before 2019\n",
    "- We looked up link by link (out of 12 newspapers selected, across the political spectrum), and looked for the tags '-chega-' and 'ventura'\n",
    "    - Filtering for '-chega-' helped reduce the number of outliers by avoiding words where 'chega' can be found (e.g., chegado, aconchega)\n",
    "    - In the case of 'ventura' the outliers weren't that many, and we had no big issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the API endpoint\n",
    "cdx_url = \"https://arquivo.pt/wayback/cdx\"\n",
    "\n",
    "# Newspapers to search\n",
    "newsp = ['cmjornal.pt/*', \n",
    "         'dn.pt/*',\n",
    "         'expresso.pt/*',\n",
    "         'folhanacional.pt/*',\n",
    "         'jn.pt/*',\n",
    "         'ionline.sapo.pt/*',   \n",
    "         'sol.sapo.pt/*',\n",
    "         'observador.pt/*',\n",
    "         'publico.pt/*',\n",
    "         'sabado.pt/*',\n",
    "         'sapo.pt/*',\n",
    "         'visao.pt/*',\n",
    "         ]\n",
    "\n",
    "# Tags to search within newspaper's links\n",
    "tags = ['-chega-', 'ventura'] # needed to add dashes before and after \"chega\" in order to avoid other words containing it \n",
    "\n",
    "# Process the response into a list\n",
    "data = []\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 2\n",
    "delay_between_requests = 5 # seconds\n",
    "\n",
    "# Function to handle requests with retries and delays\n",
    "def fetch_data_w_retries(url, params, retries=max_retries):\n",
    "    \"\"\"\n",
    "    Makes a GET request to the given URL with the given parameters, and \n",
    "    retries the request up to 'retries' times if it fails. If the request \n",
    "    fails after all retries, returns None.\n",
    "    \n",
    "    :param url: str, the URL to make the request to\n",
    "    :param params: dict, the parameters to send with the request\n",
    "    :param retries: int, the number of times to retry the request if it fails\n",
    "    :return: requests.Response, or None if the request fails after all retries\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=150)\n",
    "            response.raise_for_status() # Raise an error for 4xx or 5xx responses\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Attempt {attempt + 1} of {retries}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrieved {len(data)} records.\")\n",
    "                time.sleep(delay_between_requests) # Wait before retrying\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping.\")\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Both in the function and the rest of the script we had to manage several error inducing scenarios such as:\n",
    "- Blank error status\n",
    "- Process the data as NDJSON instead of JSON\n",
    "- Exceeding read time out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check if the response is in NDJSON format\n",
    "for i in newsp:\n",
    "    for tag in tags:\n",
    "        params = {\n",
    "        'url': i,\n",
    "        'fields': 'url,timestamp,status',\n",
    "        'from': '2022',\n",
    "        'to': '2024',\n",
    "        'filter': 'original:'+tag,\n",
    "        'output': 'json',\n",
    "        'limit': '5000'\n",
    "        }\n",
    "        \n",
    "        response = fetch_data_w_retries(cdx_url, params)\n",
    "\n",
    "        if response:\n",
    "            if response.status_code == 200:\n",
    "                if response.headers.get('Content-Type') == 'text/x-ndjson':\n",
    "                    # Process each line as a separate JSON object\n",
    "                    for line in response.text.splitlines():\n",
    "                        try:\n",
    "                            record = json.loads(line)\n",
    "\n",
    "                            status = record.get('status')\n",
    "\n",
    "                            if status == '200':\n",
    "                                data.append(record)\n",
    "                            else:\n",
    "                                if status is None:\n",
    "                                    print(f\"Record missing 'status' field: {record}\")\n",
    "                                    print(f\"Retrieved {len(data)} records.\")\n",
    "\n",
    "                                else:\n",
    "                                    print(f\"Record with status '{status}': {record}\")\n",
    "                                    print(f\"Retrieved {len(data)} records.\")\n",
    "\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error parsing line: {line}\")\n",
    "                            print(f\"JSONDecodeError: {e}\")\n",
    "                        except TypeError as e:\n",
    "                            print(f\"Unexpected data format: {line}\")\n",
    "                            print(f\"TypeError: {e}\")\n",
    "                else:\n",
    "                    print(\"Response is not in NDJSON format.\")\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve data.\")\n",
    "\n",
    "# Print the number of records retrieved\n",
    "print(f\"Retrieved {len(data)} records.\")\n",
    "\n",
    "# Insert the new data into cdx_results.json\n",
    "with open(\"cdx_results.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
