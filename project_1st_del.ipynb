{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Deliverable for FCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data mining news articles related to André Ventura / CHEGA (potentially only headlines at first)\n",
    "\t- Timeframe 2019-2024\n",
    "2. Word cloud analysis since the party foundation, assess the topics of interest throughout the years\n",
    "3. Correlation with INE (National Institute of Statistics) data with the various topics (identify real trends, or made up trends by fake news/social media)\n",
    "4. Change of the party’s position regarding the identified topics\n",
    "5. Word clouds every 5 years of the portuguese politics landscape (2000-2005-2010-2015-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to make requests from ARQUIVO's CDX API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the first part of the script we defined the newspapers to search for (yes, CDX API makes you search by a *single* url at a time), and the function that will look them up.\n",
    "\n",
    "---\n",
    "### Challenges with the CDX API\n",
    "\n",
    "The CDX API can access any stored link in **arquivo API**, but it can only be filtered with some metadata and text within the URL\n",
    "- We made use of timestamps to limit our search to the period of interest. However,  it doesn't mean that all the pages collected are from that period, as there are several snapshots from pages way before 2019\n",
    "- We looked up link by link (out of 12 newspapers selected, across the political spectrum), and looked for the tags '-chega-' and 'ventura'\n",
    "    - Filtering for '-chega-' helped reduce the number of outliers by avoiding words where 'chega' can be found (e.g., chegado, aconchega)\n",
    "    - In the case of 'ventura' the outliers weren't that many, and we had no big issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "# Define the API endpoint\n",
    "cdx_url = \"https://arquivo.pt/wayback/cdx\"\n",
    "\n",
    "# Newspapers to search\n",
    "newsp = ['cmjornal.pt/*', \n",
    "         'dn.pt/*',\n",
    "         'expresso.pt/*',\n",
    "         'folhanacional.pt/*',\n",
    "         'jn.pt/*',\n",
    "         'ionline.sapo.pt/*',   \n",
    "         'sol.sapo.pt/*',\n",
    "         'observador.pt/*',\n",
    "         'publico.pt/*',\n",
    "         'sabado.pt/*',\n",
    "         'sapo.pt/*',\n",
    "         'visao.pt/*',\n",
    "         ]\n",
    "\n",
    "# Tags to search within newspaper's links\n",
    "tags = ['-chega-', 'ventura'] # needed to add dashes before and after \"chega\" in order to avoid other words containing it \n",
    "\n",
    "# Process the response into a list\n",
    "data = []\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 2\n",
    "delay_between_requests = 5 # seconds\n",
    "\n",
    "# Function to handle requests with retries and delays\n",
    "def fetch_data_w_retries(url, params, retries=max_retries):\n",
    "    \"\"\"\n",
    "    Makes a GET request to the given URL with the given parameters, and \n",
    "    retries the request up to 'retries' times if it fails. If the request \n",
    "    fails after all retries, returns None.\n",
    "    \n",
    "    :param url: str, the URL to make the request to\n",
    "    :param params: dict, the parameters to send with the request\n",
    "    :param retries: int, the number of times to retry the request if it fails\n",
    "    :return: requests.Response, or None if the request fails after all retries\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=150)\n",
    "            response.raise_for_status() # Raise an error for 4xx or 5xx responses\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Attempt {attempt + 1} of {retries}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrieved {len(data)} records.\")\n",
    "                time.sleep(delay_between_requests) # Wait before retrying\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping.\")\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Both in the function and the rest of the script we had to manage several error inducing scenarios such as:\n",
    "- Blank error status\n",
    "- Process the data as NDJSON instead of JSON\n",
    "- Exceeding read time out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the response is in NDJSON format\n",
    "for i in newsp:\n",
    "    for tag in tags:\n",
    "        params = {\n",
    "        'url': i,\n",
    "        'fields': 'url,timestamp,status',\n",
    "        'from': '2022',\n",
    "        'to': '2024',\n",
    "        'filter': 'original:'+tag,\n",
    "        'output': 'json',\n",
    "        'limit': '5000'\n",
    "        }\n",
    "        \n",
    "        response = fetch_data_w_retries(cdx_url, params)\n",
    "\n",
    "        if response:\n",
    "            if response.status_code == 200:\n",
    "                if response.headers.get('Content-Type') == 'text/x-ndjson':\n",
    "                    # Process each line as a separate JSON object\n",
    "                    for line in response.text.splitlines():\n",
    "                        try:\n",
    "                            record = json.loads(line)\n",
    "\n",
    "                            status = record.get('status')\n",
    "\n",
    "                            if status == '200':\n",
    "                                data.append(record)\n",
    "                            else:\n",
    "                                if status is None:\n",
    "                                    print(f\"Record missing 'status' field: {record}\")\n",
    "                                    print(f\"Retrieved {len(data)} records.\")\n",
    "\n",
    "                                else:\n",
    "                                    print(f\"Record with status '{status}': {record}\")\n",
    "                                    print(f\"Retrieved {len(data)} records.\")\n",
    "\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error parsing line: {line}\")\n",
    "                            print(f\"JSONDecodeError: {e}\")\n",
    "                        except TypeError as e:\n",
    "                            print(f\"Unexpected data format: {line}\")\n",
    "                            print(f\"TypeError: {e}\")\n",
    "                else:\n",
    "                    print(\"Response is not in NDJSON format.\")\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve data.\")\n",
    "\n",
    "# Print the number of records retrieved\n",
    "print(f\"Retrieved {len(data)} records.\")\n",
    "\n",
    "# Insert the new data into cdx_results.json\n",
    "with open(\"cdx_results.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting titles from the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiving Data with CDX API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please put the file name of the .json file here so my script works with the variable 'file_name'\n",
    "\n",
    "file_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapping words out of received URLs\n",
    "\n",
    "- Open every website and webscrape the title-element in the HTML-Script\n",
    "\n",
    "- Safe the original URL as Key and the extracted words as value in a dictionary, if the title contains 'Chega' or 'CHEGA'\n",
    "- Write it into a new .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = 'test_for_scraping_cdx_results.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "titles = {}\n",
    "\n",
    "# Loop through URLs; search for the title element in HTML-Script\n",
    "for v in data:\n",
    "    url = v['url']\n",
    "    \n",
    "    try:\n",
    "        # Requesting the website and setting a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10, stream=True) # stream=True for downloading data in chunks not everything at once \n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(1)  # 1-second delay\n",
    "\n",
    "        # Initialize an empty content variable and stream content chunks\n",
    "        html_content = b\"\"\n",
    "        \n",
    "        # Stream the content until we find the closing </title> tag\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            html_content += chunk\n",
    "            if b\"</title>\" in html_content:\n",
    "                break  # Stop streaming once the <title> tag is found\n",
    "\n",
    "        # Parse only the partial content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Extract the <title> element\n",
    "        title_tag = soup.find(\"title\")\n",
    "\n",
    "        # If no <title> tag is found, skip this page, makes program faster\n",
    "        if not title_tag:\n",
    "            continue\n",
    "\n",
    "        # Extract the text from the <title> tag\n",
    "        title_text = title_tag.get_text()\n",
    "\n",
    "        # Check for \"Chega\" or \"CHEGA\" (case-sensitive check)\n",
    "        if \"Chega\" in title_text or \"CHEGA\" in title_text:\n",
    "            titles[url] = title_text  # Store the URL and title in the dictionary\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for URL: {url}\")\n",
    "        continue  # Skip to the next URL if a timeout occurs\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for URL: {url} with error: {e}\")\n",
    "        continue  # Skip to the next URL if another error occurs\n",
    "\n",
    "# Writing the dictionary to a JSON file\n",
    "with open('titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(titles, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print the results, only for debugging, to remove later\n",
    "for url, title in titles.items():\n",
    "    print(url, title)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
