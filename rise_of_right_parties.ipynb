{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook Foundations of Data Science "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiving Data with CDX API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please put the file name of the .json file here so my script works with the variable 'file_name'\n",
    "\n",
    "file_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapping words out of received URLs\n",
    "\n",
    "- Open every website and webscrape the title-element in the HTML-Script\n",
    "\n",
    "- Safe the original URL as Key and the extracted words as value in a dictionary, if the title contains 'Chega' or 'CHEGA'\n",
    "- Write it into a new .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = 'test_for_scraping_cdx_results.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "titles = {}\n",
    "\n",
    "# Loop through URLs; search for the title element in HTML-Script\n",
    "for v in data:\n",
    "    url = v['url']\n",
    "    \n",
    "    try:\n",
    "        # Requesting the website and setting a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10, stream=True) # stream=True for downloading data in chunks not everything at once \n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(1)  # 1-second delay\n",
    "\n",
    "        # Initialize an empty content variable and stream content chunks\n",
    "        html_content = b\"\"\n",
    "        \n",
    "        # Stream the content until we find the closing </title> tag\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            html_content += chunk\n",
    "            if b\"</title>\" in html_content:\n",
    "                break  # Stop streaming once the <title> tag is found\n",
    "\n",
    "        # Parse only the partial content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Extract the <title> element\n",
    "        title_tag = soup.find(\"title\")\n",
    "\n",
    "        # If no <title> tag is found, skip this page, makes program faster\n",
    "        if not title_tag:\n",
    "            continue\n",
    "\n",
    "        # Extract the text from the <title> tag\n",
    "        title_text = title_tag.get_text()\n",
    "\n",
    "        # Check for \"Chega\" or \"CHEGA\" (case-sensitive check)\n",
    "        if \"Chega\" in title_text or \"CHEGA\" in title_text:\n",
    "            titles[url] = title_text  # Store the URL and title in the dictionary\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout occurred for URL: {url}\")\n",
    "        continue  # Skip to the next URL if a timeout occurs\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for URL: {url} with error: {e}\")\n",
    "        continue  # Skip to the next URL if another error occurs\n",
    "\n",
    "# Writing the dictionary to a JSON file\n",
    "with open('titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(titles, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print the results, only for debugging, to remove later\n",
    "for url, title in titles.items():\n",
    "    print(url, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe tomorrow or at the second project submission: How to use LLM/NLP like NER or GPT to understand the content of the newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
